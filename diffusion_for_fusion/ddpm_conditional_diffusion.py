import torch
from torch import nn
from torch.nn import functional as F
import numpy as np
from diffusion_for_fusion.ddpm_fusion import Block, PositionalEmbedding



class ConditionalMLP(nn.Module):
    def __init__(self, input_dim, input_emb_size: int = 64,
                 time_emb_size: int = 128, time_emb_type: str = "sinusoidal",
                 cond_input_dim: int = 1, cond_emb_size: int = 128, input_head = False,
                 hidden_size: int = 128, hidden_layers: int = 3,
                 ):
        super().__init__()

        self.input_dim = input_dim
        self.input_emb_size = input_emb_size
        self.time_emb_size = time_emb_size
        self.time_emb_type = time_emb_type
        self.cond_input_dim = cond_input_dim
        self.cond_emb_size = cond_emb_size
        self.input_head = input_head
        self.hidden_size = hidden_size
        self.hidden_layers = hidden_layers

        # TODO: generalize this MLP to take other transforms such as sine.
        # embedding for conditional
        self.cond_mlp = nn.Linear(cond_input_dim, cond_emb_size, bias=False)

        # embedding for time
        self.time_mlp = PositionalEmbedding(time_emb_size, time_emb_type)
        self.input_head = input_head

        if input_head:
          self.input_proj = nn.Linear(input_dim, input_emb_size, bias=False) # input projection head
          concat_size = input_emb_size + time_emb_size + cond_emb_size
        else:
          concat_size = input_dim + time_emb_size + cond_emb_size

        # size of x_t generated by MLP
        output_dim = input_dim

        layers = [nn.Linear(concat_size, hidden_size), nn.GELU()]
        for _ in range(hidden_layers):
            layers.append(Block(hidden_size))
        layers.append(nn.Linear(hidden_size, output_dim))
        self.joint_mlp = nn.Sequential(*layers)

    def forward(self, x, t, c):
        c_emb = self.cond_mlp(c)
        t_emb = self.time_mlp(t)

        if self.input_head:
          x = self.input_proj(x)
        
        x = torch.cat((x, t_emb, c_emb), dim=-1)
        x = self.joint_mlp(x)
        return x


class ConditionalGaussianDiffusion(nn.Module): #rewrite it into a nn module class
    def __init__(self,
                 model, #take in the backbone denoiser
                 num_timesteps=1000,
                 beta_start=0.0001,
                 beta_end=0.02,
                 beta_schedule="linear"):
        
        super().__init__()

        self.model = model

        # schedule
        self.num_timesteps = num_timesteps
        if beta_schedule == "linear":
            scale = 1000 / num_timesteps  ##TODO: check this works!
            betas = scale * torch.linspace(
                beta_start, beta_end, num_timesteps, dtype=torch.float32)
        elif beta_schedule == "quadratic":
            betas = torch.linspace(
                beta_start ** 0.5, beta_end ** 0.5, num_timesteps, dtype=torch.float32) ** 2
            
        self.register_buffer('betas', betas)
        
        alphas = 1.0 - betas
        alphas_cumprod = torch.cumprod(alphas, axis=0)
        alphas_cumprod_prev = F.pad(alphas_cumprod[:-1], (1, 0), value=1.)
        self.register_buffer('alphas_cumprod', alphas_cumprod)
        self.register_buffer('alphas_cumprod_prev', alphas_cumprod_prev)

        # required for self.add_noise
        sqrt_alphas_cumprod = alphas_cumprod ** 0.5
        sqrt_one_minus_alphas_cumprod = (1 - alphas_cumprod) ** 0.5
        self.register_buffer('sqrt_alphas_cumprod', sqrt_alphas_cumprod)
        self.register_buffer('sqrt_one_minus_alphas_cumprod', sqrt_one_minus_alphas_cumprod)


        # required for reconstruct_x0
        sqrt_inv_alphas_cumprod = torch.sqrt(1 / alphas_cumprod)
        sqrt_inv_alphas_cumprod_minus_one = torch.sqrt(1 / alphas_cumprod - 1)
        self.register_buffer('sqrt_inv_alphas_cumprod', sqrt_inv_alphas_cumprod)
        self.register_buffer('sqrt_inv_alphas_cumprod_minus_one', sqrt_inv_alphas_cumprod_minus_one)

        # required for q_posterior
        posterior_mean_coef1 = betas * torch.sqrt(alphas_cumprod_prev) / (1. - alphas_cumprod)
        posterior_mean_coef2 = (1. - alphas_cumprod_prev) * torch.sqrt(alphas) / (1. - alphas_cumprod)
        self.register_buffer('posterior_mean_coef1', posterior_mean_coef1)
        self.register_buffer('posterior_mean_coef2', posterior_mean_coef2)

    def reconstruct_x0(self, x_t, t, noise):
        s1 = self.sqrt_inv_alphas_cumprod[t]
        s2 = self.sqrt_inv_alphas_cumprod_minus_one[t]
        s1 = s1.reshape(-1, *((1,) *(len(x_t.shape) - 1)))
        s2 = s2.reshape(-1, *((1,) *(len(noise.shape) - 1)))
        return s1 * x_t - s2 * noise

    def q_posterior(self, x_0, x_t, t):
        s1 = self.posterior_mean_coef1[t]
        s2 = self.posterior_mean_coef2[t]
        s1 = s1.reshape(-1, *((1,) *(len(x_0.shape) - 1)))
        s2 = s2.reshape(-1, *((1,) *(len(x_t.shape) - 1)))
        mu = s1 * x_0 + s2 * x_t
        return mu

    def get_variance(self, t):
        if t == 0:
            return 0

        variance = self.betas[t] * (1. - self.alphas_cumprod_prev[t]) / (1. - self.alphas_cumprod[t])
        variance = variance.clip(1e-20)
        return variance

    def step(self, model_output, timestep, sample):
        t = timestep
        pred_original_sample = self.reconstruct_x0(sample, t, model_output)
        pred_prev_sample = self.q_posterior(pred_original_sample, sample, t)

        variance = 0
        if t > 0:
            noise = torch.randn_like(model_output)
            variance = (self.get_variance(t) ** 0.5) * noise

        pred_prev_sample = pred_prev_sample + variance

        return pred_prev_sample

    def add_noise(self, x_start, x_noise, timesteps):
        s1 = self.sqrt_alphas_cumprod[timesteps]
        s2 = self.sqrt_one_minus_alphas_cumprod[timesteps]
        
        s1 = s1.reshape(-1, *((1,) *(len(x_start.shape) - 1)))
        s2 = s2.reshape(-1, *((1,) *(len(x_noise.shape) - 1)))
            
        return s1 * x_start + s2 * x_noise

    def __len__(self):
        return self.num_timesteps
    
    def forward(self, batch, noise, timesteps):
        noisy = self.add_noise(batch, noise, timesteps)
        return noisy
    
    def backward(self, noisy, timesteps, condition):
        noise_pred = self.model(noisy, timesteps, condition)
        return noise_pred
    
    def sample(self, condition):
        """Sample x_0 from the diffusion process.

        Args:
            condition (tensor): (batch_size, cond_input_dim) tensor of conditions. Conditions need not be the same
                in each row. batch_size is the number of samples you will get.

        Returns:
            sample: (batch_size, input_dim) array of samples
        """
        # TODO: push all arrays to correct device
        batch_size = condition.shape[0]
        input_shape = [batch_size, self.model.input_dim]
        timesteps = list(range(self.num_timesteps))[::-1] # reverse sampling 

        # x_T
        sample = torch.randn(input_shape)#.to(device)
        
        for i, t in enumerate(timesteps):
            t = torch.from_numpy(np.repeat(t, batch_size)).long()#.to(device)
            with torch.no_grad():
                residual = self.backward(sample, t, condition)
            sample = self.step(residual, t[0], sample)

        return sample

def init_conditional_diffusion_model_from_config(config, input_dim):
    model = ConditionalMLP(
        input_dim = input_dim,
        input_emb_size = config.input_emb_size,
        time_emb_size = config.time_emb_size,
        time_emb_type = config.time_emb_type,
        cond_input_dim = len(config.conditions),
        cond_emb_size = config.cond_emb_size,
        input_head = config.input_head,
        hidden_size = config.hidden_size,
        hidden_layers = config.hidden_layers,
        )
        
    diffusion = ConditionalGaussianDiffusion(
        model, 
        num_timesteps=config.num_timesteps,
        beta_schedule=config.beta_schedule)

    return diffusion, model


def generate_conditions_for_eval(conditions, batch_size: int = 64, from_train=True, seed=99, as_tensor=True):
    """Generate a batch of conditions to be used for sampling from the diffusion model.

    Args:
        conditions (np array): (n, cond_input_dim) array of conditions used in training.
            i.e. the training dataset of conditions.
        batch_size (int): number of samples.
        from_train (bool, optional): Whether to sample exlusively from the training set or from anywhere within the data range.
            Defaults to True; subset training data.
        seed (int, optional): numpy seed. Defaults to 99.
        as_tensor (bool, optional): return a torch tensor or numpy array. Defaults to True.

    Returns:
        np array: (batch_size, cond_input_dim) of samples.
    """
    
    np.random.seed(seed)
    n, cond_input_dim = np.shape(conditions)

    if from_train:
        idx = np.random.randint(0, n, size=batch_size )
        evals = conditions[idx]
    else:
        lb = np.min(conditions, axis=0)
        ub = np.max(conditions, axis=0)
        evals = lb + (ub - lb) * np.random.uniform(size = (batch_size, cond_input_dim))

    if as_tensor:
        evals = torch.from_numpy(evals.astype(np.float32))

    return evals
